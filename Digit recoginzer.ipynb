{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the ML model using neural networks. Creates preceptron units as class\n",
    "@author Bala Bathula\n",
    "Some of the code is borrowed from: https://rolisz.ro/2013/04/18/neural-networks-in-python/\n",
    "This NN takes the bias unit at each layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "def get_data(train=True,base_dir='./data'):\n",
    "    \"\"\"\n",
    "    This function gets the train/test data. By default the train=True. If train=False, then the test data would be returned\n",
    "    output is a dataframe (train or test)\n",
    "    \"\"\"\n",
    "    \n",
    "    if train==True:\n",
    "        return pd.read_csv(os.path.join(base_dir,'train.csv'))\n",
    "        #return pd.read_csv(os.path.join(base_dir,'ex3_data.csv')) # Use this for Coursera excercise\n",
    "    else:\n",
    "        return pd.read_csv(os.path.join(base_dir,'test.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0[[ 0.00142528]\n",
      " [ 0.99670737]\n",
      " [ 0.99672086]\n",
      " [ 0.00162224]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def derivative_tanh(x):\n",
    "    return 1.0 - np.tanh(x)**2\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with a function that can be either sigmoid, or tanh\n",
    "    Default activation unit is tanh\n",
    "    Layer size in the number of activation units in each layer (including the input and output layer)\n",
    "    # Warning: Layer size does not include the bias element. For instance if the there are 10 activation units, then total is 10+1 (+1 for bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_size = np.array([1]),activation='tanh'):\n",
    "        \"\"\"\n",
    "        Initialize weights based on input arguments. Note that no type-checking\n",
    "        is being performed here for simplicity of code.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Hidden layer size represents number of activation units in each layer\n",
    "        nbr_of_layers=len(layer_size); # This is number of hidden layers that network should have\n",
    "        \n",
    "        # Initializing weights to zero, does not work in neural networks. Because when using the back-propagation, all nodes will update to the same value.\n",
    "        # Choosing initial weights for improving the performance of the NN we use a method described in http://web.stanford.edu/class/ee373b/nninitialization.pdf\n",
    "        # Refer to the ML-Coursera Wiki notes on NN for initialization \n",
    "        # Note that in a usual NN all the hidden layers will have the same size. So we have we choose  denominator as layer_size[1]\n",
    "        # epsilon = sqrt(6/(Linput+Loutput)), where Loutput=number of units in hidden-layer\n",
    "        # epsilon=(6.0/(layer_size[0]+layer_size[1]))**(1.0/2)\n",
    "        epsilon=0.12\n",
    "        # print epsilon\n",
    "        self.weights=[] # Choose weights based on uniform distribution between [-epsilon, epsilon]\n",
    "        \n",
    "        # Add bias units to the weights except the last layer\n",
    "        for lyr in range(0,nbr_of_layers-1):\n",
    "            # Take the number of rows from the previous layer\n",
    "            # TODO: need to check this initialization from Coursera\n",
    "            # self.weights.append(np.random.random((layer_size[lyr]+1,layer_size[lyr+1]))*2*epsilon-epsilon) # .random((x,y)) gives uniformly distributed RVs between 0, 1 of size (x,y)\n",
    "            #self.weights.append((2*np.random.random((layer_size[lyr]+1,layer_size[lyr+1]))-1)*0.25) # Initialize the weights randomly between -0.25 and 0.25\n",
    "            self.weights.append(np.random.normal(0.0, layer_size[lyr]**-0.5, (layer_size[lyr]+1, layer_size[lyr+1]))) # There is no bias unit in this one\n",
    "\n",
    "        if activation=='sigmoid':\n",
    "            self.activation=sigmoid # Chooses the activation function\n",
    "            self.activation_derivative=derivative_sigmoid #  derivative of sigmoid function\n",
    "        elif activation=='tanh':\n",
    "            self.activation=tanh # Tanh function\n",
    "            self.activation_derivative=derivative_tanh # derivative tanh function\n",
    "        \n",
    "        \n",
    "        self.last_input = 0 # strength of last input\n",
    "        self.delta      = 0 # error signal\n",
    "        \n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the list of activation units for all layers for a chosen activation function\n",
    "        This is using feed-forward propagation method\n",
    "        \"\"\"\n",
    "        # If there are more than one layer, then we need to do a for-loop\n",
    "        self.last_input = values\n",
    "        all_activation_units=[values]; # Don't use self.last_input values here; As self.last_input is mutable, change in self.last_input might change activation units \n",
    "        for i in range(len(self.weights)):\n",
    "            \n",
    "            # First take the dot product of weights and values\n",
    "            strength = np.dot(self.last_input, self.weights[i])\n",
    "            \n",
    "            # Second pass the dot product to the activation unit\n",
    "            \n",
    "            self.last_input = self.activation(strength);\n",
    "            \n",
    "            # Third add the bias-units for the last_input along all rows (i.e., bias unit of column matrix)\n",
    "            if i!=len(self.weights)-1: # Do not add the bias-unit for the last layer in the network, which is y^\n",
    "                self.last_input=np.concatenate((np.ones((self.last_input.shape[0],1)),self.last_input),axis=1)\n",
    "            # Append the activation units of each layer\n",
    "            all_activation_units.append(self.last_input)\n",
    "            \n",
    "        \n",
    "        result=all_activation_units\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def update(self, values, train, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes the values as activation units and true values of y as train. Updates internal weights according to gradient descent using\n",
    "        these values and an optional learning rate, @param eta.\n",
    "        Values: activation units ()\n",
    "        train: y_true values\n",
    "        eta: learning rate\n",
    "        Updating the weights for multi-layer perceptron units has to be done using the back-propagation method\n",
    "        \"\"\"\n",
    "        # before using the values[-1] we need to reshape either values or train to np.reshape(values,(m,n)) or np.reshape(train,(m,n)) to broadcast correctly\n",
    "        # Here I choose to reshape train (which is nothing but y, this work especially, if you have more than one data point)\n",
    "        train=np.reshape(train,(train.shape[0],train.shape[1])) # This might be redunant for some cases\n",
    "        \n",
    "        # Error in the prediction is\n",
    "        error=train-values[-1];\n",
    "        \n",
    "        # Initialize the delta\n",
    "        self.delta = error\n",
    "                \n",
    "        # Elements in all_delta_units are small delta and they should be column matrices. (without bias units for hidden layers)\n",
    "        all_delta_units=[error*self.activation_derivative(values[-1])]; # Number of delta units at each layer are same as that of activation units, but without bias terms. Bias errors does not propagate\n",
    "                  \n",
    "        # First compute small deltas\n",
    "        for l in range(len(values)-2,0,-1):\n",
    "            self.delta=all_delta_units[-1].dot(self.weights[l].T)*(self.activation_derivative(values[l]))\n",
    "            self.delta=self.delta[:,1:]; # Drop the bias units \n",
    "            all_delta_units.append(self.delta)\n",
    "                    \n",
    "        # Reverse the all_delta_units array\n",
    "        all_delta_units.reverse()\n",
    "        \n",
    "        # Before computing big delta, take transpose of small deltas in each layer\n",
    "        # all_delta_units=[i.T for i in all_delta_units]\n",
    "                \n",
    "        # Compute the big-deltas, matrix size of big-delta should be same as weights matrix size \n",
    "        \n",
    "        for i in range(len(self.weights)):    \n",
    "            delta_w=np.atleast_2d(values[i].T.dot(all_delta_units[i])); # Just to make the output is 2d array\n",
    "            # (rw,cw)=self.weights[i].shape\n",
    "            #delta_w=np.reshape(delta_w,(rw,cw)) # As we know big-delta size should be same as weights, using reshape make sure the matrices are of same size\n",
    "            self.weights[i] += eta * delta_w/train.shape[0]\n",
    "     \n",
    "    def fit(self,X,y,lmbda=0.1,epochs=10000): # lmbda is the training rate (sometimes referred as eta)\n",
    "        \"\"\"\n",
    "        Takes X and y as the input arguments. lmbda and epochs are optional (learning rates and number of iterations)\n",
    "        Using stochastic gradient descent computes the updates to the weights and does the back-propagation.\n",
    "        X does not have any bias units, but the activation function of the class updates X with bias units\n",
    "        \"\"\"\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            # For each epoch using feed-forward propagation compute the activation units for the given weights\n",
    "            a=self.activate(X); # a is the list of arrays\n",
    "            # Update the weights based on the activation units\n",
    "            self.update(a,y,lmbda)\n",
    "            sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * k/float(epochs)))\n",
    "            sys.stdout.flush()\n",
    "     \n",
    "    def predict(self, X):\n",
    "        # after the fit function is called, you get the final weights, use that final weights to get a (activation units) and then get 'y_pred=a[-1]'\n",
    "        \n",
    "        a = self.activate(X)\n",
    "        \n",
    "        \n",
    "        return a[-1]\n",
    "       \n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    Run few test cases to see if the Neural network works as we intended for it to work\n",
    "    This test case is for XOR.\n",
    "    \"\"\"  \n",
    "    \n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # There are four data points in this\n",
    "    y = np.array([[0], [1], [1], [0]]) # y should be a column array\n",
    "    \n",
    "    # Assume layer size does not include the bias unit\n",
    "    layer_size=[X.shape[1],2,1]; # This is number of activation units in each  layer in the neural network should have; length of this is nothing but the number of layers\n",
    "    \n",
    "    # Concatenate ones to the input matrix (this is for the bias unit)\n",
    "    X=np.concatenate((np.ones((X.shape[0],1)),X),axis=1)\n",
    "    \n",
    "    u1 = NeuralNetwork(layer_size=layer_size,)\n",
    "    u1.fit(X,y,epochs=1000)\n",
    "    y_pred=u1.predict(X)\n",
    "    \n",
    "    return y_pred \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def digit_data_set_sklearn():\n",
    "    \"\"\"\n",
    "    Test the neural network for digit data set of sklearn\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split # Cross-validation has been moved to model_selection in 0.20 version\n",
    "    from sklearn.datasets import load_digits\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    import matplotlib.pyplot as plt \n",
    "    \n",
    "     \n",
    "    # View the digits\n",
    "    digits = load_digits()\n",
    "    plt.gray() \n",
    "    plt.matshow(digits.images[0]) \n",
    "    plt.show()\n",
    "    \n",
    "    # Start load the data\n",
    "    X = digits.data\n",
    "    y = digits.target\n",
    "    X -= X.min() # normalize the values to bring them into the range 0-1\n",
    "    X /= X.max()\n",
    "    \n",
    "    num_labels=10; # 0,1,2,...,9\n",
    "    layer_size=[X.shape[1],100,num_labels]; # Since there are 10 labels, last layer has 10 outputs, so y should be a matrix of (X.shape[0],num_labels)\n",
    "    \n",
    "    nn = NeuralNetwork(layer_size,'tanh')\n",
    "    \n",
    "    # Concatenate ones to the input matrix (this is for the bias unit)\n",
    "    X=np.concatenate((np.ones((X.shape[0],1)),X),axis=1)\n",
    "    \n",
    "    # Split the dataset into train and test sets\n",
    "    \n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y)\n",
    "    \n",
    "    \n",
    "    # We can also used labelBinarizer of sklearn instead of below logical array\n",
    "    y_train_labeled_matrix=np.zeros((X_train.shape[0],num_labels))\n",
    "    y_test_labeled_matrix=np.zeros((X_test.shape[0],num_labels))\n",
    "    \n",
    "    for c in range(0,num_labels):\n",
    "        y_train_bool=(y_train==c).astype(int) # Create an 1d array\n",
    "        y_train_labeled_matrix[:,c]=y_train_bool;\n",
    "        # Similarly for test set\n",
    "        y_test_bool=(y_test==c).astype(int) # Create an 1d array\n",
    "        y_test_labeled_matrix[:,c]=y_test_bool;\n",
    "    \n",
    "    nbr_epochs=10000\n",
    "    nn.fit(X_train,y_train_labeled_matrix,epochs=nbr_epochs)\n",
    "    \n",
    "    y_train_pred_prob=nn.predict(X_train); # Shape of y_pred_prob is same that of y_labeled_matrix. This is probability of the data to be identified as a digit\n",
    "    \n",
    "    print (y_train_pred_prob)\n",
    "    \n",
    "    outdir='results'\n",
    "    try:\n",
    "        os.makedirs(outdir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    np.savetxt('./%s/nn_prob_sklearn_digit_.csv'%(outdir),y_train_pred_prob,delimiter=',')\n",
    "    \n",
    "    train_pred_digits=np.argmax(y_train_pred_prob,axis=1)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./%s/nn_train_sklearn_digit_.csv'%(outdir),train_pred_digits,delimiter=',')\n",
    "    print(train_pred_digits.shape)\n",
    "    \n",
    "    print(\"Training set accuracy: {0}\".format(np.mean(train_pred_digits==y_train)*100))\n",
    "    \n",
    "    # Get the test set prediction\n",
    "    y_test_pred_prob=nn.predict(X_test)\n",
    "    test_pred_digits=np.argmax(y_test_pred_prob,axis=1)\n",
    "    \n",
    "    df_test_pred=pd.DataFrame(test_pred_digits,index=range(1,test_pred_digits.shape[0]+1),columns=['Label'])\n",
    "    df_test_pred.index.names=['ImageId']\n",
    "    \n",
    "    df_test_pred.to_csv('%s/nn_test_results_%d_sklearn_digit_.csv'%(outdir,nbr_epochs))\n",
    "    \n",
    "    print (confusion_matrix(y_test,test_pred_digits))\n",
    "    print (classification_report(y_test,test_pred_digits))\n",
    "    \n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21b16a8ec18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH0iI4gMJUXwgof8A4C6Y4wlBav8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21b16be0240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0[[-0.06965261  0.16897833 -0.00138125 ..., -0.00263079  0.21398317\n",
      "  -0.11303368]\n",
      " [-0.09796091 -0.00133879 -0.0090153  ..., -0.15989643  0.02283692\n",
      "   0.06653438]\n",
      " [-0.060233    0.20332574 -0.07882075 ..., -0.01939499  0.88046216\n",
      "  -0.23148222]\n",
      " ..., \n",
      " [-0.08788308  0.09042691 -0.11542168 ...,  0.01154851  0.04615755\n",
      "  -0.12471681]\n",
      " [-0.0273582   0.22603051 -0.13605809 ..., -0.03525684  0.83906127\n",
      "  -0.21837863]\n",
      " [-0.11258263  0.9000396   0.07291809 ..., -0.08265999  0.01616933\n",
      "   0.00422702]]\n",
      "(1347,)\n",
      "Training set accuracy: 98.51521900519673\n",
      "[[46  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 42  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 45  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 43  0  0  0  0  1  0]\n",
      " [ 0  0  0  0 57  0  0  0  0  0]\n",
      " [ 0  0  0  0  1 40  0  0  1  2]\n",
      " [ 0  1  0  0  0  0 38  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 47  1  0]\n",
      " [ 0  1  0  0  0  0  0  0 40  0]\n",
      " [ 0  0  0  0  0  1  0  0  3 40]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        46\n",
      "          1       0.95      1.00      0.98        42\n",
      "          2       1.00      1.00      1.00        45\n",
      "          3       1.00      0.98      0.99        44\n",
      "          4       0.98      1.00      0.99        57\n",
      "          5       0.98      0.91      0.94        44\n",
      "          6       1.00      0.97      0.99        39\n",
      "          7       1.00      0.98      0.99        48\n",
      "          8       0.87      0.98      0.92        41\n",
      "          9       0.95      0.91      0.93        44\n",
      "\n",
      "avg / total       0.97      0.97      0.97       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "digit_data_set_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
